"""
AquaBrain AI Engine - Multi-Model Support
==========================================

×× ×•×¢ AI ×¢× ×ª××™×›×” ×‘:
- Claude (Anthropic) - Haiku, Sonnet, Opus
- Gemini (Google) - Flash, Pro

×©×™××•×©:
    from services.ai_engine import ask_ai, ask_aquabrain

    # ×©××œ×” ×¢× Claude (×‘×¨×™×¨×ª ××—×“×œ)
    response = ask_ai("××” ×–×” NFPA 13?")

    # ×©××œ×” ×¢× Gemini
    response = ask_ai("××” ×–×” NFPA 13?", provider="gemini")

    # ×©××œ×” ×”× ×“×¡×™×ª ×¢× AquaBrain
    response = ask_aquabrain("×—×©×‘ ××•×‘×“×Ÿ ×œ×—×¥ ×‘×¦×™× ×•×¨ 2 ××™× ×¥'")
"""

import os
import requests
from typing import List, Dict, Optional, Literal
from dotenv import load_dotenv
from enum import Enum

load_dotenv()


# ============================================================
# Configuration
# ============================================================

class AIProvider(str, Enum):
    CLAUDE = "claude"
    GEMINI = "gemini"
    GPT = "gpt"  # ×œ×¢×ª×™×“


class ClaudeModel(str, Enum):
    HAIKU = "claude-3-5-haiku-20241022"      # ××”×™×¨ ×•×–×•×œ
    SONNET = "claude-sonnet-4-20250514"       # ×××•×–×Ÿ - ××•××œ×¥!
    OPUS = "claude-opus-4-20250514"           # ×”×›×™ ×—×–×§


class GeminiModel(str, Enum):
    FLASH = "gemini-2.0-flash"       # ××”×™×¨ - 1500/day
    FLASH_25 = "gemini-2.5-flash"    # ×××•×–×Ÿ - 500/day
    PRO = "gemini-2.5-pro"           # ×—×–×§ - 25/day


# Default models
DEFAULT_PROVIDER = "gemini"  # Gemini as default (free tier)
DEFAULT_CLAUDE_MODEL = ClaudeModel.SONNET.value
DEFAULT_GEMINI_MODEL = GeminiModel.FLASH_25.value  # 500 req/day


# ============================================================
# Claude Client
# ============================================================

class ClaudeClient:
    """×§×œ×™×™× ×˜ ×œ-Anthropic Claude API."""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("âŒ ANTHROPIC_API_KEY ×œ× ×”×•×’×“×¨!")
        self.base_url = "https://api.anthropic.com/v1"
        self.api_version = "2023-06-01"

    def generate(
        self,
        prompt: str,
        model: str = DEFAULT_CLAUDE_MODEL,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> str:
        """×™×¦×™×¨×ª ×ª×•×›×Ÿ ×¢× Claude."""

        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.api_key,
            "anthropic-version": self.api_version
        }

        payload = {
            "model": model,
            "max_tokens": max_tokens,
            "messages": [{"role": "user", "content": prompt}]
        }

        if system_prompt:
            payload["system"] = system_prompt

        if temperature != 1.0:
            payload["temperature"] = temperature

        response = requests.post(
            f"{self.base_url}/messages",
            headers=headers,
            json=payload,
            timeout=120
        )
        response.raise_for_status()

        data = response.json()
        return data["content"][0]["text"]

    def chat(
        self,
        messages: List[Dict[str, str]],
        model: str = DEFAULT_CLAUDE_MODEL,
        system_prompt: Optional[str] = None
    ) -> str:
        """×¦'××˜ ×¨×‘-×ª×•×¨×•×ª ×¢× Claude."""

        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.api_key,
            "anthropic-version": self.api_version
        }

        # ×”××¨×ª ×¤×•×¨××˜ ×”×•×“×¢×•×ª
        formatted_messages = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "assistant"
            formatted_messages.append({
                "role": role,
                "content": msg["content"]
            })

        payload = {
            "model": model,
            "max_tokens": 4096,
            "messages": formatted_messages
        }

        if system_prompt:
            payload["system"] = system_prompt

        response = requests.post(
            f"{self.base_url}/messages",
            headers=headers,
            json=payload,
            timeout=120
        )
        response.raise_for_status()

        data = response.json()
        return data["content"][0]["text"]


# ============================================================
# Gemini Client
# ============================================================

class GeminiClient:
    """×§×œ×™×™× ×˜ ×œ-Google Gemini API."""

    # ××•×“×œ×™× ×–××™× ×™× (Free Tier)
    MODELS = {
        "pro": "gemini-2.5-pro",        # ×”×›×™ ×—×–×§ - 25 ×‘×§×©×•×ª/×™×•×
        "flash": "gemini-2.5-flash",    # ×××•×–×Ÿ - 500 ×‘×§×©×•×ª/×™×•× (×‘×¨×™×¨×ª ××—×“×œ)
        "fast": "gemini-2.0-flash",     # ×”×›×™ ××”×™×¨ - 1500 ×‘×§×©×•×ª/×™×•×
    }

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("âŒ GEMINI_API_KEY ×œ× ×”×•×’×“×¨!")
        self.base_url = "https://generativelanguage.googleapis.com/v1beta"

    def generate(
        self,
        prompt: str,
        model: str = DEFAULT_GEMINI_MODEL,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> str:
        """×™×¦×™×¨×ª ×ª×•×›×Ÿ ×¢× Gemini."""

        url = f"{self.base_url}/models/{model}:generateContent?key={self.api_key}"

        # ×‘× ×™×™×ª ×”×¤×¨×•××¤×˜ ×¢× system instructions
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"[System Instructions]\n{system_prompt}\n\n[User Query]\n{prompt}"

        payload = {
            "contents": [{
                "parts": [{"text": full_prompt}]
            }],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
            }
        }

        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()

        data = response.json()
        return data["candidates"][0]["content"]["parts"][0]["text"]

    def chat(
        self,
        messages: List[Dict[str, str]],
        model: str = DEFAULT_GEMINI_MODEL,
        system_prompt: Optional[str] = None
    ) -> str:
        """×¦'××˜ ×¨×‘-×ª×•×¨×•×ª ×¢× Gemini."""

        url = f"{self.base_url}/models/{model}:generateContent?key={self.api_key}"

        contents = []

        if system_prompt:
            contents.append({
                "role": "user",
                "parts": [{"text": f"[System Instructions]\n{system_prompt}"}]
            })
            contents.append({
                "role": "model",
                "parts": [{"text": "××•×‘×Ÿ. ××¤×¢×œ ×œ×¤×™ ×”×”×•×¨××•×ª."}]
            })

        for msg in messages:
            role = "user" if msg["role"] == "user" else "model"
            contents.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })

        payload = {"contents": contents}

        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()

        data = response.json()
        return data["candidates"][0]["content"]["parts"][0]["text"]


# ============================================================
# Unified AI Interface
# ============================================================

_claude_client = None
_gemini_client = None


def get_claude_client() -> ClaudeClient:
    """××—×–×™×¨ instance ×™×—×™×“ ×©×œ Claude client."""
    global _claude_client
    if _claude_client is None:
        _claude_client = ClaudeClient()
    return _claude_client


def get_gemini_client() -> GeminiClient:
    """××—×–×™×¨ instance ×™×—×™×“ ×©×œ Gemini client."""
    global _gemini_client
    if _gemini_client is None:
        _gemini_client = GeminiClient()
    return _gemini_client


def ask_ai(
    prompt: str,
    provider: Literal["claude", "gemini"] = DEFAULT_PROVIDER,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7
) -> str:
    """
    ×©××œ×” ××—×•×“×” ×œ×›×œ ×”-AI providers.

    Args:
        prompt: ×”×©××œ×”
        provider: "claude" ××• "gemini" (×‘×¨×™×¨×ª ××—×“×œ: gemini)
        model: ×©× ×”××•×“×œ (××•×¤×¦×™×•× ×œ×™ - ×™×™×§×— ×‘×¨×™×¨×ª ××—×“×œ)
        system_prompt: ×”×•×¨××•×ª ××¢×¨×›×ª
        temperature: ×¨××ª ×™×¦×™×¨×ª×™×•×ª (0.0-1.0)

    ×“×•×’×××•×ª:
        # Gemini (×‘×¨×™×¨×ª ××—×“×œ - 500 req/day free)
        response = ask_ai("××” ×–×” NFPA 13?")

        # Claude
        response = ask_ai("××” ×–×” NFPA 13?", provider="claude")

        # ××•×“×œ ×¡×¤×¦×™×¤×™
        response = ask_ai("...", provider="gemini", model="gemini-2.5-pro")
    """

    if provider == "claude":
        client = get_claude_client()
        model = model or DEFAULT_CLAUDE_MODEL
        return client.generate(prompt, model, system_prompt, temperature)

    elif provider == "gemini":
        client = get_gemini_client()
        model = model or DEFAULT_GEMINI_MODEL
        return client.generate(prompt, model, system_prompt, temperature)

    else:
        raise ValueError(f"Provider ×œ× × ×ª××š: {provider}")


def chat_ai(
    messages: List[Dict[str, str]],
    provider: Literal["claude", "gemini"] = DEFAULT_PROVIDER,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None
) -> str:
    """×¦'××˜ ×¨×‘-×ª×•×¨×•×ª ××—×•×“."""

    if provider == "claude":
        client = get_claude_client()
        model = model or DEFAULT_CLAUDE_MODEL
        return client.chat(messages, model, system_prompt)

    elif provider == "gemini":
        client = get_gemini_client()
        model = model or DEFAULT_GEMINI_MODEL
        return client.chat(messages, model, system_prompt)

    else:
        raise ValueError(f"Provider ×œ× × ×ª××š: {provider}")


# ============================================================
# AquaBrain Specialized Functions
# ============================================================

AQUABRAIN_SYSTEM_PROMPT = """××ª×” AquaBrain - ××•××—×” ×œ×”× ×“×¡×ª ××¢×¨×›×•×ª ×›×™×‘×•×™ ××© ×•×¡×¤×¨×™× ×§×œ×¨×™×.

×”×ª××—×•×™×•×ª:
- ×ª×§×Ÿ NFPA 13 (×××¨×™×§××™) - ×›×œ ×”×’×¨×¡××•×ª
- ×ª×§×Ÿ ×™×©×¨××œ×™ ×ª"×™ 1596
- ×—×™×©×•×‘×™ ×”×™×“×¨××•×œ×™×§×” (Hazen-Williams, Darcy-Weisbach)
- ×ª×›× ×•×Ÿ ××¢×¨×›×•×ª ×¡×¤×¨×™× ×§×œ×¨×™× (Wet, Dry, Deluge, Pre-action)
- × ×™×ª×•×— ×§×‘×¦×™ IFC/BIM
- ×¡×™×•×•×’×™ ×¡×™×›×•×Ÿ (Light Hazard, Ordinary Hazard Group 1/2, Extra Hazard Group 1/2)
- ×—×™×©×•×‘×™ ×¦×¤×™×¤×•×ª ×•×©×˜×— ×¤×¢×•×œ×” (Design Area)
- ×‘×—×™×¨×ª ×¨××©×™ ×¡×¤×¨×™× ×§×œ×¨×™× (K-factor, RTI, Temperature Rating)

×”× ×—×™×•×ª:
1. ×¢× ×” ×‘×¢×‘×¨×™×ª ××§×¦×•×¢×™×ª ×•×‘×¨×•×¨×”
2. ×”×¡×‘×¨ ×—×™×©×•×‘×™× ×©×œ×‘ ××—×¨ ×©×œ×‘
3. ×¦×™×™×Ÿ ×ª××™×“ ××ª ×”×ª×§×Ÿ ×”×¨×œ×•×•× ×˜×™ ×•××¡×¤×¨ ×”×¡×¢×™×£
4. ×× ×—×¡×¨ ××™×“×¢ ×§×¨×™×˜×™ - ×©××œ ×©××œ×•×ª ×”×‘×”×¨×”
5. ×”×¦×’ × ×•×¡×—××•×ª ×•×™×—×™×“×•×ª ××™×“×”
6. ×”×ª×™×™×—×¡ ×œ×’×•×¨××™ ×‘×˜×™×—×•×ª ×•××¨×•×•×—×™×"""


def ask_aquabrain(
    question: str,
    provider: Literal["claude", "gemini"] = DEFAULT_PROVIDER,
    model: Optional[str] = None
) -> str:
    """
    ×©××œ×” ×”× ×“×¡×™×ª ×œ-AquaBrain.

    ×“×•×’×××•×ª:
        response = ask_aquabrain("×—×©×‘ ××•×‘×“×Ÿ ×œ×—×¥ ×‘×¦×™× ×•×¨ 2 ××™× ×¥' ×‘××•×¨×š 30 ××˜×¨")
        response = ask_aquabrain("××” ×”×“×¨×™×©×•×ª ×œ×¡×¤×¨×™× ×§×œ×¨×™× ×‘×—× ×™×•×Ÿ ×ª×ª-×§×¨×§×¢×™?")
        response = ask_aquabrain("...", provider="claude")  # ×¢× Claude
    """
    return ask_ai(
        prompt=question,
        provider=provider,
        model=model,
        system_prompt=AQUABRAIN_SYSTEM_PROMPT,
        temperature=0.3  # ×™×•×ª×¨ ×“×™×™×§× ×™ ×œ××©×™××•×ª ×”× ×“×¡×™×•×ª
    )


def analyze_ifc_element(
    element_data: Dict,
    analysis_type: str = "compliance",
    provider: Literal["claude", "gemini"] = DEFAULT_PROVIDER
) -> str:
    """
    × ×™×ª×•×— ××œ×× ×˜ IFC.

    Args:
        element_data: × ×ª×•× ×™ ×”××œ×× ×˜ ××§×•×‘×¥ IFC
        analysis_type: ×¡×•×’ ×”× ×™×ª×•×— (compliance, hydraulic, spacing)
        provider: ×”-AI provider
    """
    import json

    prompt = f"""× ×ª×— ××ª ××œ×× ×˜ ×”-IFC ×”×‘×:

```json
{json.dumps(element_data, indent=2, ensure_ascii=False)}
```

×¡×•×’ × ×™×ª×•×—: {analysis_type}

×‘×¦×¢:
1. ×–×”×” ××ª ×¡×•×’ ×”××œ×× ×˜
2. ×‘×“×•×§ ×ª××™××•×ª ×œ×ª×§×Ÿ ×”×¨×œ×•×•× ×˜×™
3. ×”×¦×’ ×××¦××™× ×•×‘×¢×™×•×ª ×¤×•×˜× ×¦×™××œ×™×•×ª
4. ×”×¦×¢ ×ª×™×§×•× ×™× ×× × ×“×¨×©"""

    return ask_aquabrain(prompt, provider=provider)


# ============================================================
# Legacy Functions (×ª××™××•×ª ×œ××—×•×¨)
# ============================================================

def ask_gemini(
    prompt: str,
    model: str = DEFAULT_GEMINI_MODEL,
    temperature: float = 0.7
) -> str:
    """×ª××™××•×ª ×œ××—×•×¨ - ××©×ª××© ×‘-ask_ai ×¢× Gemini."""
    return ask_ai(prompt, provider="gemini", model=model, temperature=temperature)


def ask_claude(
    prompt: str,
    model: str = DEFAULT_CLAUDE_MODEL,
    temperature: float = 0.7
) -> str:
    """×ª××™××•×ª ×œ××—×•×¨ - ××©×ª××© ×‘-ask_ai ×¢× Claude."""
    return ask_ai(prompt, provider="claude", model=model, temperature=temperature)


def chat_with_gemini(
    messages: List[Dict[str, str]],
    model: str = DEFAULT_GEMINI_MODEL
) -> str:
    """×ª××™××•×ª ×œ××—×•×¨ - ×¦'××˜ ×¢× Gemini."""
    return chat_ai(messages, provider="gemini", model=model)


def get_client() -> GeminiClient:
    """×ª××™××•×ª ×œ××—×•×¨ - ××—×–×™×¨ Gemini client."""
    return get_gemini_client()


# ============================================================
# Exports
# ============================================================

__all__ = [
    # Providers & Models
    'AIProvider',
    'ClaudeModel',
    'GeminiModel',
    # Clients
    'ClaudeClient',
    'GeminiClient',
    'get_claude_client',
    'get_gemini_client',
    # Unified Interface
    'ask_ai',
    'chat_ai',
    # AquaBrain
    'ask_aquabrain',
    'analyze_ifc_element',
    'AQUABRAIN_SYSTEM_PROMPT',
    # Legacy
    'ask_gemini',
    'ask_claude',
    'chat_with_gemini',
    'get_client',
]


# ============================================================
# Testing
# ============================================================

def test_connection(provider: str = "all"):
    """×‘×“×™×§×ª ×—×™×‘×•×¨ ×œ-AI providers."""

    print("=" * 60)
    print("ğŸ§  AquaBrain - ×‘×“×™×§×ª ×—×™×‘×•×¨ AI (Multi-Model)")
    print("=" * 60)

    results = {}

    # Test Gemini (default, free tier)
    if provider in ["all", "gemini"]:
        print("\n[Gemini] ×‘×•×“×§ ×—×™×‘×•×¨...")
        try:
            response = ask_ai(
                "Say 'Gemini connected!' in exactly 2 words.",
                provider="gemini"
            )
            print(f"    âœ… Gemini: {response.strip()}")
            results["gemini"] = True
        except Exception as e:
            print(f"    âŒ Gemini: {e}")
            results["gemini"] = False

    # Test Claude (requires API key)
    if provider in ["all", "claude"]:
        print("\n[Claude] ×‘×•×“×§ ×—×™×‘×•×¨...")
        try:
            response = ask_ai(
                "Say 'Claude connected!' in exactly 2 words.",
                provider="claude"
            )
            print(f"    âœ… Claude: {response.strip()}")
            results["claude"] = True
        except Exception as e:
            error_msg = str(e)
            if "ANTHROPIC_API_KEY" in error_msg:
                print(f"    âš ï¸  Claude: API key not configured (optional)")
            else:
                print(f"    âŒ Claude: {e}")
            results["claude"] = False

    # Test AquaBrain with working provider
    if any(results.values()):
        print("\n[AquaBrain] ×‘×•×“×§ ××•××—×™×•×ª...")
        working_provider = "gemini" if results.get("gemini") else "claude"
        try:
            response = ask_aquabrain(
                "××” ×”×§×•×˜×¨ ×”××™× ×™××œ×™ ×œ×¦×™× ×•×¨ ×¢× ×£ ×¡×¤×¨×™× ×§×œ×¨×™× ×œ×¤×™ NFPA 13?",
                provider=working_provider
            )
            print(f"    âœ… AquaBrain ({working_provider}):")
            print(f"       {response.strip()[:150]}...")
        except Exception as e:
            print(f"    âŒ AquaBrain: {e}")

    print("\n" + "=" * 60)

    # Summary
    working = [k for k, v in results.items() if v]
    if working:
        print(f"âœ… ××—×•×‘×¨: {', '.join(working)}")
        print(f"   ×‘×¨×™×¨×ª ××—×“×œ: {DEFAULT_PROVIDER} ({DEFAULT_GEMINI_MODEL})")
    else:
        print("âŒ ××™×Ÿ ×—×™×‘×•×¨ ×œ××£ provider")

    print("=" * 60)
    return results


if __name__ == "__main__":
    test_connection()
